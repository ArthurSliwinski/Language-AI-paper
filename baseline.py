"""
In this notebook, we set up a baseline model for our research on predicting political leaning based on used vocabulary in the
Reddit tweets. This model will serve as a comparison for our lr model.
"""

# necessary imports
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.feature_extraction.text import CountVectorizer
import re
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GroupShuffleSplit



# load the dataset

data = pd.read_csv(".\lai-data\political_leaning.csv")
print(data)

## Lets take a look at class-imbalance first

class_counts = data['political_leaning'].value_counts() # calculate counts
class_proportions = data['political_leaning'].value_counts(normalize=True)  # proportions

class_distribution = pd.DataFrame({
    'Count': class_counts,
    'Proportion': class_proportions})

class_distribution.reset_index(inplace=True)
class_distribution.rename(columns={'index': 'political_leaning'}, inplace=True)

print(f'\nClass distribution of all tweets')
print(class_distribution)

# now we group by users to see class-imbalance per user, since the number of tweets generated by users might differ across different political leaning

# for this, first we assume that a user only makes one type of tweets from the perspective of political leaning,
# but we need to test this assumption first

user_political_leanings = data.groupby('auhtor_ID')['political_leaning'].nunique()
users_with_multiple_leanings = user_political_leanings[user_political_leanings > 1] # check if any user has multiple political leanings


if not users_with_multiple_leanings.empty:
    print("\nUsers with multiple political leanings found:")
    print(users_with_multiple_leanings)
else:
    print("\nAll users have consistent political leanings.")  # we found that all users had consistent political leanings

# now we can recompute class imbalance counting the number of users

user_class = data.groupby('auhtor_ID')['political_leaning'].first()

user_class_counts = user_class.value_counts() # count
user_class_proportions = user_class.value_counts(normalize=True) # proportion

user_class_distribution = pd.DataFrame({
    'Count': user_class_counts,
    'Proportion': user_class_proportions
})
user_class_distribution.reset_index(inplace=True)
user_class_distribution.rename(columns={'index': 'political_leaning'}, inplace=True)

print("\nClass distribution by users:")
print(user_class_distribution)  # we found that there is no significant difference in activitity based on political leaning

# So, if we consider a model predicting the most frequent class (center) only as the baseline, it would perform with an
# appr. 44% accuracy

## Next, we explore how a random classifier assigning labels randomly based on the class distributions would perform

np.random.seed(600)  # for reproducibility

classes = class_proportions.index  # class labels (center, right, left)
probabilities = class_proportions.values  # class probabilities

random_predictions = np.random.choice(classes, size=len(data), p=probabilities) # the model for the random predictions based on the underlying probabilities

accuracy = accuracy_score(data["political_leaning"], random_predictions)    # Evaluation metrics
report = classification_report(data["political_leaning"], random_predictions, target_names=classes)
conf_matrix = confusion_matrix(data["political_leaning"], random_predictions)
print(f"\n\nAccuracy of Random Classifier: {accuracy}")
print("\nClassification Report:")
print(report)
print("\nConfusion Matrix:")
print(conf_matrix)
# the random classifier achieved an appr. 35% accuracy, with balanced predictions across all classes

## Now, we create our third baseline, namely a shallow Decision Tree

# first, we tokenize the texts into unigrams
# since the posts contain a lot of special characters we cannot interpret, such as cirillic alphabet, we filter every word containing any other character than the latin aplhabet
def preprocess_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'\b[^a-z]+\b', ' ', text)  # Remove words with non-Latin characters (no accents or numbers)
    text = re.sub(r'\b\d+\w*\b', ' ', text)  # Remove words containing numbers (e.g., '100k', '10s')
    return text

# now we use the vectorizer
vectorizer = CountVectorizer(preprocessor= preprocess_text, stop_words='english', min_df=100, max_df= 10000)  # lowercase and remove english frequent words, including all unigrams we get memory error
X = vectorizer.fit_transform(data['post'])  # tokenized text matrix
y = data['political_leaning']  # target classes (center, right, left)

# inspect the tokenized features
feature_names = vectorizer.get_feature_names_out()
X_dense = pd.DataFrame(X.toarray(), columns=feature_names)  # convert to a dense df for inspection

print("Feature Names (Unigrams):", feature_names)
print("\nTokenized Text Matrix (first 5 rows):")
print(X_dense.head())

# second, using this preprocessed data we train the shallow decision tree
#for this we start by doing  a train-test split, but making sure that tweets corresponding to the same author remain in only one set, avoiding data leakegsa

user_ids = data['auhtor_ID']

# Initialize GroupShuffleSplit
gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)

# Split the data into train and test sets
for train_idx, test_idx in gss.split(X, y, groups=user_ids):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
# Initialize the decision tree classifier with max_depth = 3 for a shallow tree
clf = DecisionTreeClassifier(max_depth=3, random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"\n\nAccuracy of Decision Tree: {accuracy:.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
